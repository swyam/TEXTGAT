{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gatgcn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q9ZT8ENoj8J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9c0fa7c2-1b8b-44c8-947b-ce10841cefe3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG3hKB4vLsyd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "df288a7a-78fc-4f0e-a4cb-e71a1ad4d8db"
      },
      "source": [
        "\n",
        "!pip uninstall tensorboard\n",
        "!pip install  tf-nightly-2.0-preview"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping tensorboard as it is not installed.\u001b[0m\n",
            "Requirement already satisfied: tf-nightly-2.0-preview in /usr/local/lib/python3.6/dist-packages (2.0.0.dev20191002)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.8.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.2.2)\n",
            "Requirement already satisfied: tb-nightly<2.2.0a0,>=2.1.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (2.1.0a20191206)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.11.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.17.4)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.33.6)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.25.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.1.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator-2.0-preview in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (2.0.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (3.10.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly-2.0-preview) (42.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly-2.0-preview) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly-2.0-preview) (2.21.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly-2.0-preview) (0.16.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly-2.0-preview) (1.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly-2.0-preview) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tf-nightly-2.0-preview) (2.8.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly-2.0-preview) (1.3.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly-2.0-preview) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly-2.0-preview) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly-2.0-preview) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly-2.0-preview) (2.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly-2.0-preview) (0.2.7)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly-2.0-preview) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly-2.0-preview) (3.1.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly-2.0-preview) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly-2.0-preview) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5Da5yHgPuBI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "cbdd1243-4001-4920-d5ec-7a82ec50feff"
      },
      "source": [
        "!pip install grpcio==1.25.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: grpcio==1.25.0 in /usr/local/lib/python3.6/dist-packages (1.25.0)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from grpcio==1.25.0) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upncAxVYNcrU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils, datasets\n",
        "\n",
        "from tensorflow import summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzCmO-AKNhba",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "75e4f157-bb4a-4eb0-b0ac-04d0d4a38d51"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxOsNzT2ookc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51fcc361-67c7-44aa-d04f-65942bf597b3"
      },
      "source": [
        "cd ../GAT/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/GAT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qS5jWZjjO8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#layers\n",
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        \n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        #self.weight.data.uniform_(-stdv, stdv)\n",
        "        nn.init.xavier_uniform_(self.weight.data, gain=1)\n",
        "        if self.bias is not None:\n",
        "            #nn.init.xavier_uniform_(self.bias.data, gain=1)\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "+ str(self.out_features) + ')'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtslrm89jVVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#GCN\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        #\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, adj)\n",
        "        y=x.clone()\n",
        "        return F.log_softmax(x, dim=1),y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqzAWP3-o24D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class GraphAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.alpha = alpha\n",
        "        self.concat = concat\n",
        "\n",
        "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
        "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "        self.theta=nn.parameter(torch.nn.init.normal_(tensor, mean=0, std=1))\n",
        "\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        h = torch.mm(input, self.W)\n",
        "        N = h.size()[0]\n",
        "\n",
        "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
        "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
        "\n",
        "        zero_vec = -9e15*torch.ones_like(e)\n",
        "        \n",
        "        #attention = torch.where(adj > 0, e, zero_vec)\n",
        "        attention = torch.where(adj > self.theta, e, zero_vec)\n",
        "        attention = F.softmax(attention, dim=1)  #normalization of attention score\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "        h_prime = torch.matmul(attention, h)\n",
        "\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            return h_prime\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
        "\n",
        "\n",
        "class SpecialSpmmFunction(torch.autograd.Function):\n",
        "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, indices, values, shape, b):\n",
        "        assert indices.requires_grad == False\n",
        "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
        "        ctx.save_for_backward(a, b)\n",
        "        ctx.N = shape[0]\n",
        "        return torch.matmul(a, b)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        a, b = ctx.saved_tensors\n",
        "        grad_values = grad_b = None\n",
        "        if ctx.needs_input_grad[1]:\n",
        "            grad_a_dense = grad_output.matmul(b.t())\n",
        "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
        "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
        "        if ctx.needs_input_grad[3]:\n",
        "            grad_b = a.t().matmul(grad_output)\n",
        "        return None, grad_values, None, grad_b\n",
        "\n",
        "\n",
        "class SpecialSpmm(nn.Module):\n",
        "    def forward(self, indices, values, shape, b):\n",
        "        return SpecialSpmmFunction.apply(indices, values, shape, b)\n",
        "\n",
        "    \n",
        "class SpGraphAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "        super(SpGraphAttentionLayer, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.alpha = alpha\n",
        "        self.concat = concat\n",
        "\n",
        "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
        "        nn.init.xavier_normal_(self.W.data, gain=1)\n",
        "                \n",
        "        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
        "        nn.init.xavier_normal_(self.a.data, gain=1)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "        self.theta = torch.empty(1, 1)\n",
        "        torch.nn.init.normal_(self.theta, mean=0.0, std=1.0)\n",
        "        #self.theta=nn.parameter(torch.nn.init.normal_(theta, mean=0, std=1))\n",
        "\n",
        "        self.special_spmm = SpecialSpmm()\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        dv = 'cuda' if input.is_cuda else 'cpu'\n",
        "\n",
        "        N = input.size()[0]\n",
        "\n",
        "        #zero_vec = -9e15*torch.ones_like(e)\n",
        "\n",
        "        #nonzero_mask = np.array(adj[adj.nonzero()] < 3)\n",
        "\n",
        "        assert not torch.isnan(input).any()\n",
        "        assert not torch.isnan(self.W).any()\n",
        "\n",
        "\n",
        "        edge = adj.nonzero().t()\n",
        "        #print(edge)\n",
        "\n",
        "        h = torch.mm(input, self.W)\n",
        "        # h: N x out\n",
        "        assert not torch.isnan(h).any()\n",
        "\n",
        "        # Self-attention on the nodes - Shared attention mechanism\n",
        "        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n",
        "        # edge: 2*D x E\n",
        "\n",
        "        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))\n",
        "\n",
        "        assert not torch.isnan(edge_e).any()\n",
        "        # edge_e: E\n",
        "\n",
        "        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,1), device=dv))\n",
        "        #print(e_rowsum[755:765])\n",
        "        # e_rowsum: N x 1\n",
        "        #print(np.sort(e_rowsum.clone().detach().numpy()))\n",
        "\n",
        "        assert not torch.isnan(edge_e).any()#\n",
        "\n",
        "        edge_e = self.dropout(edge_e)\n",
        "        # edge_e: E\n",
        "\n",
        "        assert not torch.isnan(edge_e).any()#\n",
        "\n",
        "        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n",
        "\n",
        "        zero_vec = -9e15*torch.ones_like(e_rowsum)\n",
        "\n",
        "        e_rowsum=torch.where(e_rowsum > 0, e_rowsum, zero_vec)\n",
        "        \n",
        "        assert not torch.isnan(h_prime).any()\n",
        "        # h_prime: N x out\n",
        "        #print(\"X\",h_prime[760])\n",
        "        #print(h_prime)\n",
        "        h_prime = h_prime.div(e_rowsum)\n",
        "        #print(\"X\",h_prime[760])\n",
        "        # h_prime: N x out\n",
        "        #xx=(h_prime!=h_prime)\n",
        "        #print((xx==True).nonzero())\n",
        "\n",
        "        assert not torch.isnan(h_prime).any()\n",
        "\n",
        "        if self.concat:\n",
        "            # if this layer is not last layer,\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            # if this layer is last layer,\n",
        "            return h_prime\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUk5i19ho9dD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#from layers import GraphAttentionLayer, SpGraphAttentionLayer\n",
        "\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
        "        \"\"\"Dense version of GAT.\"\"\"\n",
        "        super(GAT, self).__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
        "        for i, attention in enumerate(self.attentions):\n",
        "            self.add_module('attention_{}'.format(i), attention)\n",
        "\n",
        "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        y=x.clone()\n",
        "        x = F.elu(self.out_att(x, adj))\n",
        "        return F.log_softmax(x, dim=1),y\n",
        "\n",
        "\n",
        "class SpGAT(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
        "        \"\"\"Sparse version of GAT.\"\"\"\n",
        "        super(SpGAT, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        \n",
        "\n",
        "        self.attentions = [SpGraphAttentionLayer(nfeat, \n",
        "                                                 nhid, \n",
        "                                                 dropout=dropout, \n",
        "                                                 alpha=alpha, \n",
        "                                                 concat=True) for _ in range(nheads)]\n",
        "        for i, attention in enumerate(self.attentions):\n",
        "            self.add_module('attention_{}'.format(i), attention)\n",
        "\n",
        "        self.out_att = SpGraphAttentionLayer(nhid * nheads, \n",
        "                                             nclass, \n",
        "                                             dropout=dropout, \n",
        "                                             alpha=alpha, \n",
        "                                             concat=False)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        \n",
        "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        y=x.clone()\n",
        "        x = F.elu(self.out_att(x, adj))\n",
        "        return F.log_softmax(x, dim=1),y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmge3aT0IIPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"combine GCN and GAT\"\n",
        "\n",
        "\n",
        "class SpGATGCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid1,nhid2, nclass, dropout, alpha, nheads):\n",
        "        \"\"\"Sparse version of GAT.\"\"\"\n",
        "        \n",
        "        super(SpGATGCN, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid1)\n",
        "        #self.add_module('Graph convolution layer_{ }'.format(1),self.gc1)\n",
        "        #self.gc2 = GraphConvolution(nhid1+50, nhid1)\n",
        "\n",
        "        self.attentions = [SpGraphAttentionLayer(nhid1, \n",
        "                                                 nhid2, \n",
        "                                                 dropout=dropout, \n",
        "                                                 alpha=alpha, \n",
        "                                                 concat=True) for _ in range(nheads)]\n",
        "        for i, attention in enumerate(self.attentions):\n",
        "            self.add_module('attention_{}'.format(i), attention)\n",
        "\n",
        "        self.out_att = SpGraphAttentionLayer(nhid2 * nheads, \n",
        "                                             nclass, \n",
        "                                             dropout=dropout, \n",
        "                                             alpha=alpha, \n",
        "                                             concat=False)\n",
        "        #for i, attention in enumerate(self.out_att):\n",
        "        #self.add_module('attention_outer_{}'.format(1), self.out_att)\n",
        "        \n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        assert not torch.isnan(x).any()\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        assert not torch.isnan(x).any()\n",
        "        \n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        assert not torch.isnan(x).any()\n",
        "        \n",
        "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
        "        assert not torch.isnan(x).any()\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        assert not torch.isnan(x).any()\n",
        "        y=x.clone()\n",
        "        x = F.elu(self.out_att(x, adj))\n",
        "        return F.log_softmax(x, dim=1),y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBhuCfqW1DAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"combine GCN and GAT\"\n",
        "\n",
        "\n",
        "class SpGATGCN1(nn.Module):\n",
        "    def __init__(self, nfeat, nhid1,nhid2, nclass, dropout, alpha, nheads):\n",
        "        \"\"\"Sparse version of GAT.\"\"\"\n",
        "        \n",
        "        super(SpGATGCN1, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid1)\n",
        "        #self.add_module('Graph convolution layer_{ }'.format(1),self.gc1)\n",
        "        #self.gc2 = GraphConvolution(nhid1+50, nhid1)\n",
        "\n",
        "        self.attentions1 = [SpGraphAttentionLayer(nhid1, \n",
        "                                                 nhid2*4, \n",
        "                                                 dropout=dropout, \n",
        "                                                 alpha=alpha, \n",
        "                                                 concat=True) for _ in range(nheads)]\n",
        "\n",
        "        self.attentions2 = [SpGraphAttentionLayer(nhid2*4*nheads, \n",
        "                                                 nhid2, \n",
        "                                                 dropout=dropout, \n",
        "                                                 alpha=alpha, \n",
        "                                                 concat=True) for _ in range(nheads)]\n",
        "        for i, attention in enumerate(self.attentions1):\n",
        "            self.add_module('attention1_{}'.format(i), attention)\n",
        "        for i, attention in enumerate(self.attentions2):\n",
        "            self.add_module('attention2_{}'.format(i), attention)\n",
        "\n",
        "        self.out_att = SpGraphAttentionLayer(nhid2 * nheads, \n",
        "                                             nclass, \n",
        "                                             dropout=dropout, \n",
        "                                             alpha=alpha, \n",
        "                                             concat=False)\n",
        "        #for i, attention in enumerate(self.out_att):\n",
        "        #self.add_module('attention_outer_{}'.format(1), self.out_att)\n",
        "        \n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        assert not torch.isnan(x).any()\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        assert not torch.isnan(x).any()\n",
        "        \n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        assert not torch.isnan(x).any()\n",
        "        \n",
        "        x = torch.cat([att(x, adj) for att in self.attentions1], dim=1)\n",
        "        assert not torch.isnan(x).any()\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = torch.cat([att(x, adj) for att in self.attentions2], dim=1)\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        y=x.clone()\n",
        "        assert not torch.isnan(x).any()\n",
        "        x = F.elu(self.out_att(x, adj))\n",
        "        return F.log_softmax(x, dim=1),y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pc9wj7WEpKpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
        "import sys\n",
        "import re\n",
        "import torch\n",
        "\n",
        "def parse_index_file(filename):\n",
        "    \"\"\"Parse index file.\"\"\"\n",
        "    index = []\n",
        "    for line in open(filename):\n",
        "        index.append(int(line.strip()))\n",
        "    return index\n",
        "\n",
        "\n",
        "def sample_mask(idx, l):\n",
        "    \"\"\"Create mask.\"\"\"\n",
        "    mask = np.zeros(l)\n",
        "    mask[idx] = 1\n",
        "    return np.array(mask, dtype=np.bool)\n",
        "\n",
        "\n",
        "def load_corpus1(dataset_str):\n",
        "\n",
        "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'adj']\n",
        "    objects = []\n",
        "    for i in range(len(names)):\n",
        "        with open(\"/content/drive/My Drive/Colab Notebooks/text_gat/data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
        "            if sys.version_info > (3, 0):\n",
        "                objects.append(pkl.load(f, encoding='latin1'))\n",
        "            else:\n",
        "                objects.append(pkl.load(f))\n",
        "\n",
        "    x, y, tx, ty, allx, ally, adj = tuple(objects)\n",
        "    # print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
        "    print(\"SHAPE ALLX\",allx.shape,\"shape tx\",tx.shape)\n",
        "    \n",
        "    features = sp.vstack((allx, tx))\n",
        "    features=sp.eye(features.shape[0])\n",
        "    ##\n",
        "    features = normalize_features(features)\n",
        "    ##remember\n",
        "    print(\"feature shape\",features.shape)\n",
        "    labels = np.vstack((ally, ty))\n",
        "    \n",
        "    # print(len(labels))\n",
        "\n",
        "    train_idx_orig = parse_index_file(\n",
        "        \"/content/drive/My Drive/Colab Notebooks/text_gat/data/{}.train.index\".format(dataset_str))\n",
        "    train_size = len(train_idx_orig)\n",
        "\n",
        "    val_size = train_size - x.shape[0]\n",
        "    test_size = tx.shape[0]\n",
        "\n",
        "    idx_train = range(len(y))\n",
        "    idx_val = range(len(y), len(y) + val_size)\n",
        "    idx_test = range(allx.shape[0], allx.shape[0] + test_size)\n",
        "    \n",
        "    labels=np.argmax(labels,axis=1)\n",
        "\n",
        "\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "    #symmetric features\n",
        "\n",
        "    adj=normalize_adj(adj+ sp.eye(adj.shape[0]))\n",
        "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    #labels = torch.LongTensor(np.where(labels)[1])\n",
        "    labels=torch.LongTensor(np.array(labels))\n",
        "    \n",
        "    \n",
        "    print(\"Xz\",labels.shape)\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "    #labels=np.argmax(labels, axis=0)\n",
        "\n",
        "    return adj, features,labels,idx_train,idx_val,idx_test\n",
        "\n",
        "def sparse_to_tuple(sparse_mx):\n",
        "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
        "    def to_tuple(mx):\n",
        "        if not sp.isspmatrix_coo(mx):\n",
        "            mx = mx.tocoo()\n",
        "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "        values = mx.data\n",
        "        shape = mx.shape\n",
        "        return coords, values, shape\n",
        "\n",
        "    if isinstance(sparse_mx, list):\n",
        "        for i in range(len(sparse_mx)):\n",
        "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "    else:\n",
        "        sparse_mx = to_tuple(sparse_mx)\n",
        "\n",
        "    return sparse_mx\n",
        "\n",
        "def normalize_features(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def preprocess_features(features):\n",
        "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "    rowsum = np.array(features.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    # return sparse_to_tuple(features)\n",
        "    return features.A\n",
        "\n",
        "\n",
        "def normalize_adj_(adj):#kipf\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "def normalize_adj(mx):#velicovic\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
        "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
        "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
        "\n",
        "\n",
        "def preprocess_adj(adj):\n",
        "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
        "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "    # return sparse_to_tuple(adj_normalized)\n",
        "    return adj_normalized.A\n",
        "\n",
        "\n",
        "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
        "    \"\"\"Construct feed dictionary.\"\"\"\n",
        "    feed_dict = dict()\n",
        "    feed_dict.update({placeholders['labels']: labels})\n",
        "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
        "    feed_dict.update({placeholders['features']: features})\n",
        "    feed_dict.update({placeholders['support'][i]: support[i]\n",
        "                      for i in range(len(support))})\n",
        "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
        "    return feed_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def loadWord2Vec(filename):\n",
        "    \"\"\"Read Word Vectors\"\"\"\n",
        "    vocab = []\n",
        "    embd = []\n",
        "    word_vector_map = {}\n",
        "    file = open(filename, 'r')\n",
        "    for line in file.readlines():\n",
        "        row = line.strip().split(' ')\n",
        "        if(len(row) > 2):\n",
        "            vocab.append(row[0])\n",
        "            vector = row[1:]\n",
        "            length = len(vector)\n",
        "            for i in range(length):\n",
        "                vector[i] = float(vector[i])\n",
        "            embd.append(vector)\n",
        "            word_vector_map[row[0]] = vector\n",
        "    print_log('Loaded Word Vectors!')\n",
        "    file.close()\n",
        "    return vocab, embd, word_vector_map\n",
        "\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for all datasets except for SST.\n",
        "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip().lower()\n",
        "\n",
        "\n",
        "import datetime\n",
        "def print_log(msg='', end='\\n'):\n",
        "    now = datetime.datetime.now()\n",
        "    t = str(now.year) + '/' + str(now.month) + '/' + str(now.day) + ' ' \\\n",
        "      + str(now.hour).zfill(2) + ':' + str(now.minute).zfill(2) + ':' + str(now.second).zfill(2)\n",
        "\n",
        "    if isinstance(msg, str):\n",
        "        lines = msg.split('\\n')\n",
        "    else:\n",
        "        lines = [msg]\n",
        "        \n",
        "    for line in lines:\n",
        "        if line == lines[-1]:\n",
        "            print('[' + t + '] ' + str(line), end=end)\n",
        "        else: \n",
        "            print('[' + t + '] ' + str(line))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f6u2rrarfpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def load_corpus(dataset_str):\n",
        "    \"\"\"\n",
        "    Loads input corpus from gcn/data directory\n",
        "\n",
        "    ind.dataset_str.x => the feature vectors of the training docs as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.tx => the feature vectors of the test docs as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training docs/words\n",
        "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.y => the one-hot labels of the labeled training docs as numpy.ndarray object;\n",
        "    ind.dataset_str.ty => the one-hot labels of the test docs as numpy.ndarray object;\n",
        "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
        "    ind.dataset_str.adj => adjacency matrix of word/doc nodes as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.train.index => the indices of training docs in original doc list.\n",
        "\n",
        "    All objects above must be saved using python pickle module.\n",
        "\n",
        "    :param dataset_str: Dataset name\n",
        "    :return: All data input files loaded (as well the training/test data).\n",
        "    \"\"\"\n",
        "\n",
        "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'adj']\n",
        "    objects = []\n",
        "    for i in range(len(names)):\n",
        "        with open(\"../data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
        "            if sys.version_info > (3, 0):\n",
        "                objects.append(pkl.load(f, encoding='latin1'))\n",
        "            else:\n",
        "                objects.append(pkl.load(f))\n",
        "\n",
        "    x, y, tx, ty, allx, ally, adj = tuple(objects)\n",
        "    # print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
        "    # print(\"SHAPE ALLX\",allx.shape,\"shape x\",tx.shape)\n",
        "    features = sp.vstack((allx, tx)).tolil()\n",
        "    print(\"feature shape\",features.shape)\n",
        "    features=sp.eye(features.shape[0])\n",
        "    ##\n",
        "    features = normalize_features(features)\n",
        "\n",
        "    labels = np.vstack((ally, ty))\n",
        "    #print(labels)\n",
        "\n",
        "    train_idx_orig = parse_index_file(\n",
        "        \"../data/{}.train.index\".format(dataset_str))\n",
        "    train_size = len(train_idx_orig)\n",
        "\n",
        "    val_size = train_size - x.shape[0]\n",
        "    test_size = tx.shape[0]\n",
        "\n",
        "    idx_train = range(len(y))\n",
        "    idx_val = range(len(y), len(y) + val_size)\n",
        "    idx_test = range(allx.shape[0], allx.shape[0] + test_size)\n",
        "\n",
        "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
        "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
        "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
        "\n",
        "    y_train = np.zeros(labels.shape)\n",
        "    y_val = np.zeros(labels.shape)\n",
        "    y_test = np.zeros(labels.shape)\n",
        "    y_train[train_mask, :] = labels[train_mask, :]\n",
        "    y_val[val_mask, :] = labels[val_mask, :]\n",
        "    y_test[test_mask, :] = labels[test_mask, :]\n",
        "\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "    adj=normalize_adj(adj+ sp.eye(adj.shape[0]))\n",
        "\n",
        "    #adj = torch.FloatTensor(np.array(adj.todense()))\n",
        "    #features = torch.FloatTensor(np.array(features.todense()))\n",
        "\n",
        "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOpo72NO4UXf",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZUFoUxxpRx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "\n",
        "\n",
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
        "    return labels_onehot\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def normalize_adj(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
        "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
        "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
        "\n",
        "\n",
        "def normalize_features(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtfsGBkfplIm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "a8176959-1cb5-445b-f966-36beb51ee3b0"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "# Training settings\n",
        "parser = argparse.ArgumentParser()\n",
        "#parser = parser.parse_args(args=[])\n",
        "parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training.')\n",
        "parser.add_argument('--fastmode', action='store_true', default=False, help='Validate during training pass.')\n",
        "parser.add_argument('--sparse', action='store_true', default=True, help='GAT with sparse version or not.')\n",
        "parser.add_argument('--seed', type=int, default=82, help='Random seed.')\n",
        "parser.add_argument('--epochs', type=int, default=10000, help='Number of epochs to train.')\n",
        "parser.add_argument('--lr', type=float, default=0.01, help='Initial learning rate.')\n",
        "parser.add_argument('--weight_decay', type=float, default=0.00001, help='Weight decay (L2 loss on parameters).')\n",
        "parser.add_argument('--hidden', type=int, default=8, help='Number of hidden units.')\n",
        "parser.add_argument('--nb_heads', type=int, default=8, help='Number of head attentions.')\n",
        "parser.add_argument('--dropout', type=float, default=0.5, help='Dropout rate (1 - keep probability).')\n",
        "parser.add_argument('--alpha', type=float, default=0.2, help='Alpha for the leaky_relu.')\n",
        "parser.add_argument('--patience', type=int, default=100, help='Patience')\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "random.seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "# Load data\n",
        "#adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
        "#print(features)\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_corpus1(\"R8\")\n",
        "print(args.hidden)\n",
        "# Model and optimizer\n",
        "if args.sparse:\n",
        "    print(\"sparse\")\n",
        "    \"\"\"model = GCN(nfeat=features.shape[1], \n",
        "                nhid=200, \n",
        "                nclass=int(labels.max()) + 1, \n",
        "                dropout=args.dropout)\"\"\"\n",
        "    model = SpGATGCN(nfeat=features.shape[1], \n",
        "                nhid1=200, \n",
        "                nhid2=args.hidden,\n",
        "                nclass=int(labels.max()) + 1, \n",
        "                dropout=args.dropout, \n",
        "                nheads=args.nb_heads, \n",
        "                alpha=args.alpha)\n",
        "    \"\"\" model = SpGATGCN1(nfeat=features.shape[1], \n",
        "                nhid1=200, \n",
        "                nhid2=args.hidden,\n",
        "                nclass=int(labels.max()) + 1, \n",
        "                dropout=args.dropout, \n",
        "                nheads=args.nb_heads, \n",
        "                alpha=args.alpha)\"\"\"\n",
        "else:\n",
        "    model = GAT(nfeat=features.shape[1], \n",
        "                nhid=args.hidden, \n",
        "                nclass=int(labels.max()) + 1, \n",
        "                dropout=args.dropout, \n",
        "                nheads=args.nb_heads, \n",
        "                alpha=args.alpha)\n",
        "print(model)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SHAPE ALLX (13173, 300) shape tx (2189, 300)\n",
            "feature shape (15362, 15362)\n",
            "Xz torch.Size([15362])\n",
            "8\n",
            "sparse\n",
            "SpGATGCN(\n",
            "  (gc1): GraphConvolution (15362 -> 200)\n",
            "  (attention_0): SpGraphAttentionLayer (200 -> 8)\n",
            "  (attention_1): SpGraphAttentionLayer (200 -> 8)\n",
            "  (attention_2): SpGraphAttentionLayer (200 -> 8)\n",
            "  (attention_3): SpGraphAttentionLayer (200 -> 8)\n",
            "  (attention_4): SpGraphAttentionLayer (200 -> 8)\n",
            "  (attention_5): SpGraphAttentionLayer (200 -> 8)\n",
            "  (attention_6): SpGraphAttentionLayer (200 -> 8)\n",
            "  (attention_7): SpGraphAttentionLayer (200 -> 8)\n",
            "  (out_att): SpGraphAttentionLayer (64 -> 8)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaeCct3FILhQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "b686ea17-3288-48b1-a3f9-185649f7d071"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), \n",
        "                       lr=args.lr, \n",
        "                       weight_decay=args.weight_decay)\n",
        "\n",
        "if args.cuda:\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    adj = adj.cuda()\n",
        "    labels = labels.cuda()\n",
        "    idx_train = idx_train.cuda()\n",
        "    idx_val = idx_val.cuda()\n",
        "    idx_test = idx_test.cuda()\n",
        "\n",
        "features, adj, labels = Variable(features), Variable(adj), Variable(labels)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output,y = model(features, adj)\n",
        "    yyyy=output[idx_train]\n",
        "    print(yyyy.shape)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "\n",
        "    loss_train.backward()\n",
        "    #torch.nn.utils.clip_grad_value_(model.parameters(),1)\n",
        "    \n",
        "    optimizer.step()\n",
        "\n",
        "    if not args.fastmode:\n",
        "        # Evaluate validation set performance separately,\n",
        "        # deactivates dropout during validation run.\n",
        "        model.eval()\n",
        "        output,y = model(features, adj)\n",
        "\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test=accuracy(output[idx_test], labels[idx_test])\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.data.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.data.item()),\n",
        "          'loss_test: {:.4f}'.format(loss_test.data.item()),\n",
        "          'acc_test: {:.4f}'.format(acc_test.data.item())\n",
        "          'time: {:.4f}'.format(time.time() - t))\n",
        "    with train_summary_writer.as_default():\n",
        "           summary.scalar('val_loss', loss_val.data.item(), step=epoch)\n",
        "           summary.scalar('train_loss', loss_train.data.item(), step=epoch)\n",
        "           summary.scalar('train_acc', acc_train.data.item(), step=epoch)\n",
        "           summary.scalar('val_acc', acc_val.data.item(), step=epoch)\n",
        "    return loss_val.data.item(),acc_val.data.item(),acc_train.data.item(),loss_train.data.item(),loss_test.data.item(),acc_test.data.item()\n",
        "\n",
        "\n",
        "def compute_test(best_epoch):\n",
        "    model.eval()\n",
        "    output,y = model(features, adj)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.data.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.data.item()))\n",
        "    with test_summary_writer.as_default():\n",
        "        summary.scalar('loss', loss_test.data.item(), step=best_epoch)\n",
        "        summary.scalar('accuracy', acc_test, step=best_epoch)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Testing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-36-91ca3959babe>\"\u001b[0;36m, line \u001b[0;32m49\u001b[0m\n\u001b[0;31m    'time: {:.4f}'.format(time.time() - t))\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki9tt1XYPS6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "current_time = str(datetime.datetime.now().timestamp())\n",
        "train_log_dir = 'logs/tensorboard/train/' + current_time\n",
        "test_log_dir = 'logs/tensorboard/test/' + current_time\n",
        "train_summary_writer = summary.create_file_writer(train_log_dir)\n",
        "test_summary_writer = summary.create_file_writer(test_log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwoOxFnNPW65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs/tensorboard  #tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGLCKhsK5YtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train model\n",
        "t_total = time.time()\n",
        "loss_values = []\n",
        "bad_counter = 0\n",
        "best = args.epochs + 1\n",
        "best_epoch = 0\n",
        "metrics=[]\n",
        "for epoch in range(args.epochs):\n",
        "    loss_val,acc_val,acc_train,loss_train,loss_test,acc_test=train(epoch)\n",
        "    metrics.append((loss_val,acc_val,acc_train,loss_train,loss_test,acc_test))\n",
        "\n",
        "    loss_values.append(loss_val)\n",
        "    torch.save(metrics,'metricsrR8.ax')\n",
        "\n",
        "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
        "    if loss_values[-1] < best:\n",
        "        best = loss_values[-1]\n",
        "        best_epoch = epoch\n",
        "        bad_counter = 0\n",
        "    else:\n",
        "        bad_counter += 1\n",
        "\n",
        "    if bad_counter == args.patience:\n",
        "        break\n",
        "\n",
        "    files = glob.glob('*.pkl')\n",
        "    for file in files:\n",
        "        epoch_nb = int(file.split('.')[0])\n",
        "        if epoch_nb < best_epoch:\n",
        "            os.remove(file)\n",
        "\n",
        "files = glob.glob('*.pkl')\n",
        "for file in files:\n",
        "    epoch_nb = int(file.split('.')[0])\n",
        "    if epoch_nb > best_epoch:\n",
        "        os.remove(file)\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzbzl9Lzn0Cv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.DataFrame(adj.clone().detach().numpy())\n",
        "#sorted(df,reverse=False)\n",
        "pd.DataFrame(df.sum(0)).to_csv(\"Filr.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p1iZ_-2yeKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import networkx as nx\n",
        "G=nx.Graph()\n",
        "G=nx.from_numpy_matrix(adj.clone().detach().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnCuZFtzzJNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(G.number_of_edges())\n",
        "print(G.number_of_nodes())\n",
        "df=pd.DataFrame(G.degree())\n",
        "sorted(df)\n",
        "df.to_csv(\"degr.csv\")\n",
        "len(list(nx.connected_components(G)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTTIQsKUy9Dg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nx.draw(G)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-TYwDAwTQUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pkg_resources\n",
        "\n",
        "for entry_point in pkg_resources.iter_entry_points('tensorboard_plugins'):\n",
        "    print(entry_point.dist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7AfXaXfT0b5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 926
        },
        "outputId": "103700d4-095f-4b68-c3a8-c674b464f3bc"
      },
      "source": [
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metric=torch.load('metricsrR8.ax')\n",
        "df=pd.DataFrame(metric,columns=[\"a\",\"b\",\"c\",\"d\",\"e\",\"f\"])\n",
        "col=df.columns\n",
        "df1=df[col[0]]\n",
        "df2=df[col[1]]\n",
        "df3=df[col[2]]\n",
        "df4=df[col[3]]\n",
        "df5=df[col[4]]\n",
        "df6=df[col[5]]\n",
        "fig = plt.figure()\n",
        "ax = plt.axes()\n",
        "\n",
        "x = range(len(df.values))\n",
        "plt.plot(x,df1.values,label=\"val_loss\")\n",
        "plt.plot(x,df4.values,label=\"train_loss\")\n",
        "plt.plot(x,df5.values,label=\"test_loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.legend()\n",
        "plt.savefig(\"{}loss.png\".format(\"R8\"))\n",
        "plt.show()\n",
        "\n",
        "plt.plot(x,df2.values,label=\"val_acc\")\n",
        "plt.plot(x,df3.values,label=\"train_acc\")\n",
        "plt.plot(x,df6.values,label=\"test_acc\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"acc\")\n",
        "plt.legend()\n",
        "plt.savefig(\"{}acc.png\".format(\"R8\"))\n",
        "plt.show()\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLBAYY4zjLBAYY4zj/h+I2oyaMRLO0gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "      <th>d</th>\n",
              "      <th>e</th>\n",
              "      <th>f</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.746217</td>\n",
              "      <td>0.492701</td>\n",
              "      <td>0.520762</td>\n",
              "      <td>2.023114</td>\n",
              "      <td>1.745112</td>\n",
              "      <td>0.494746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.508350</td>\n",
              "      <td>0.492701</td>\n",
              "      <td>0.520356</td>\n",
              "      <td>1.743895</td>\n",
              "      <td>1.502996</td>\n",
              "      <td>0.494746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.386072</td>\n",
              "      <td>0.492701</td>\n",
              "      <td>0.520559</td>\n",
              "      <td>1.508666</td>\n",
              "      <td>1.372194</td>\n",
              "      <td>0.494746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.374712</td>\n",
              "      <td>0.492701</td>\n",
              "      <td>0.519749</td>\n",
              "      <td>1.391821</td>\n",
              "      <td>1.349190</td>\n",
              "      <td>0.494746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.404371</td>\n",
              "      <td>0.501825</td>\n",
              "      <td>0.520559</td>\n",
              "      <td>1.395021</td>\n",
              "      <td>1.366972</td>\n",
              "      <td>0.505254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>464</th>\n",
              "      <td>0.156971</td>\n",
              "      <td>0.956204</td>\n",
              "      <td>0.955439</td>\n",
              "      <td>0.134386</td>\n",
              "      <td>0.166665</td>\n",
              "      <td>0.948835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>465</th>\n",
              "      <td>0.378502</td>\n",
              "      <td>0.892336</td>\n",
              "      <td>0.952400</td>\n",
              "      <td>0.147905</td>\n",
              "      <td>0.337835</td>\n",
              "      <td>0.905436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>466</th>\n",
              "      <td>0.298307</td>\n",
              "      <td>0.910584</td>\n",
              "      <td>0.909459</td>\n",
              "      <td>0.264869</td>\n",
              "      <td>0.223472</td>\n",
              "      <td>0.942896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>467</th>\n",
              "      <td>0.231586</td>\n",
              "      <td>0.928832</td>\n",
              "      <td>0.920802</td>\n",
              "      <td>0.216684</td>\n",
              "      <td>0.194608</td>\n",
              "      <td>0.953860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>468</th>\n",
              "      <td>0.209778</td>\n",
              "      <td>0.917883</td>\n",
              "      <td>0.939437</td>\n",
              "      <td>0.183994</td>\n",
              "      <td>0.238811</td>\n",
              "      <td>0.916857</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>469 rows  6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            a         b         c         d         e         f\n",
              "0    1.746217  0.492701  0.520762  2.023114  1.745112  0.494746\n",
              "1    1.508350  0.492701  0.520356  1.743895  1.502996  0.494746\n",
              "2    1.386072  0.492701  0.520559  1.508666  1.372194  0.494746\n",
              "3    1.374712  0.492701  0.519749  1.391821  1.349190  0.494746\n",
              "4    1.404371  0.501825  0.520559  1.395021  1.366972  0.505254\n",
              "..        ...       ...       ...       ...       ...       ...\n",
              "464  0.156971  0.956204  0.955439  0.134386  0.166665  0.948835\n",
              "465  0.378502  0.892336  0.952400  0.147905  0.337835  0.905436\n",
              "466  0.298307  0.910584  0.909459  0.264869  0.223472  0.942896\n",
              "467  0.231586  0.928832  0.920802  0.216684  0.194608  0.953860\n",
              "468  0.209778  0.917883  0.939437  0.183994  0.238811  0.916857\n",
              "\n",
              "[469 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN1YEHSCqEMN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "outputId": "8f6abae7-810d-4aa9-c22e-9f706cbcd81a"
      },
      "source": [
        "col=df.columns\n",
        "df[ df[col[2]]>0.96]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "      <th>d</th>\n",
              "      <th>e</th>\n",
              "      <th>f</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>411</th>\n",
              "      <td>0.160867</td>\n",
              "      <td>0.952555</td>\n",
              "      <td>0.960705</td>\n",
              "      <td>0.133428</td>\n",
              "      <td>0.174393</td>\n",
              "      <td>0.943353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416</th>\n",
              "      <td>0.153378</td>\n",
              "      <td>0.943431</td>\n",
              "      <td>0.962528</td>\n",
              "      <td>0.128381</td>\n",
              "      <td>0.172727</td>\n",
              "      <td>0.940155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>418</th>\n",
              "      <td>0.166502</td>\n",
              "      <td>0.939781</td>\n",
              "      <td>0.960502</td>\n",
              "      <td>0.130869</td>\n",
              "      <td>0.191954</td>\n",
              "      <td>0.935587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>0.133268</td>\n",
              "      <td>0.959854</td>\n",
              "      <td>0.960097</td>\n",
              "      <td>0.132334</td>\n",
              "      <td>0.162700</td>\n",
              "      <td>0.946094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>0.125548</td>\n",
              "      <td>0.961679</td>\n",
              "      <td>0.961110</td>\n",
              "      <td>0.119711</td>\n",
              "      <td>0.154413</td>\n",
              "      <td>0.949292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>0.143294</td>\n",
              "      <td>0.954380</td>\n",
              "      <td>0.960502</td>\n",
              "      <td>0.126168</td>\n",
              "      <td>0.156790</td>\n",
              "      <td>0.948378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>0.154215</td>\n",
              "      <td>0.939781</td>\n",
              "      <td>0.963541</td>\n",
              "      <td>0.123559</td>\n",
              "      <td>0.161163</td>\n",
              "      <td>0.945637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>445</th>\n",
              "      <td>0.149559</td>\n",
              "      <td>0.939781</td>\n",
              "      <td>0.960097</td>\n",
              "      <td>0.122820</td>\n",
              "      <td>0.162785</td>\n",
              "      <td>0.947008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>450</th>\n",
              "      <td>0.143820</td>\n",
              "      <td>0.948905</td>\n",
              "      <td>0.962933</td>\n",
              "      <td>0.117656</td>\n",
              "      <td>0.144467</td>\n",
              "      <td>0.957515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>453</th>\n",
              "      <td>0.157165</td>\n",
              "      <td>0.950730</td>\n",
              "      <td>0.963541</td>\n",
              "      <td>0.117495</td>\n",
              "      <td>0.186397</td>\n",
              "      <td>0.937871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>456</th>\n",
              "      <td>0.132528</td>\n",
              "      <td>0.948905</td>\n",
              "      <td>0.963136</td>\n",
              "      <td>0.114356</td>\n",
              "      <td>0.161179</td>\n",
              "      <td>0.946551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>459</th>\n",
              "      <td>0.128848</td>\n",
              "      <td>0.958029</td>\n",
              "      <td>0.961313</td>\n",
              "      <td>0.124122</td>\n",
              "      <td>0.154962</td>\n",
              "      <td>0.951576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>460</th>\n",
              "      <td>0.127937</td>\n",
              "      <td>0.954380</td>\n",
              "      <td>0.961515</td>\n",
              "      <td>0.116576</td>\n",
              "      <td>0.156736</td>\n",
              "      <td>0.951119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>461</th>\n",
              "      <td>0.141901</td>\n",
              "      <td>0.947080</td>\n",
              "      <td>0.966579</td>\n",
              "      <td>0.109926</td>\n",
              "      <td>0.167187</td>\n",
              "      <td>0.943353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>462</th>\n",
              "      <td>0.126909</td>\n",
              "      <td>0.952555</td>\n",
              "      <td>0.960907</td>\n",
              "      <td>0.117667</td>\n",
              "      <td>0.143837</td>\n",
              "      <td>0.956144</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            a         b         c         d         e         f\n",
              "411  0.160867  0.952555  0.960705  0.133428  0.174393  0.943353\n",
              "416  0.153378  0.943431  0.962528  0.128381  0.172727  0.940155\n",
              "418  0.166502  0.939781  0.960502  0.130869  0.191954  0.935587\n",
              "427  0.133268  0.959854  0.960097  0.132334  0.162700  0.946094\n",
              "431  0.125548  0.961679  0.961110  0.119711  0.154413  0.949292\n",
              "434  0.143294  0.954380  0.960502  0.126168  0.156790  0.948378\n",
              "435  0.154215  0.939781  0.963541  0.123559  0.161163  0.945637\n",
              "445  0.149559  0.939781  0.960097  0.122820  0.162785  0.947008\n",
              "450  0.143820  0.948905  0.962933  0.117656  0.144467  0.957515\n",
              "453  0.157165  0.950730  0.963541  0.117495  0.186397  0.937871\n",
              "456  0.132528  0.948905  0.963136  0.114356  0.161179  0.946551\n",
              "459  0.128848  0.958029  0.961313  0.124122  0.154962  0.951576\n",
              "460  0.127937  0.954380  0.961515  0.116576  0.156736  0.951119\n",
              "461  0.141901  0.947080  0.966579  0.109926  0.167187  0.943353\n",
              "462  0.126909  0.952555  0.960907  0.117667  0.143837  0.956144"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaR_XfgCNAX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "output,y = model(features, adj)\n",
        "train_=y[idx_train]\n",
        "val_=y[idx_val]\n",
        "test_=y[idx_test]\n",
        "docs=torch.cat([train_,val_,test_],dim=0)\n",
        "idx=torch.cat([idx_train,idx_val,idx_test],dim=0)\n",
        "print(idx.shape)\n",
        "labell=labels[idx]\n",
        "\n",
        "\n",
        "label=labels[idx_test]\n",
        "\n",
        "\n",
        "#**********TSNE**************************************************************\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "fea = TSNE(n_components=2).fit_transform(test_.detach().numpy())\n",
        "#pdf = PdfPages(data_set + '_gcn_doc_test.pdf')\n",
        "cls = np.unique(labels)\n",
        "\n",
        "# cls=range(10)\n",
        "fea_num = [fea[label == i] for i in cls]\n",
        "for i, f in enumerate(fea_num):\n",
        "    if cls[i] in range(10):\n",
        "        plt.scatter(f[:, 0], f[:, 1], label=cls[i], marker='+')\n",
        "    else:\n",
        "        plt.scatter(f[:, 0], f[:, 1], label=cls[i])\n",
        "plt.legend(ncol=5, loc='upper center', bbox_to_anchor=(0.48, -0.08), fontsize=11)\n",
        "# plt.ylim([-20,35])\n",
        "# plt.title(md_file)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"R8test.png\")\n",
        "plt.show()\n",
        "#pdf.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}